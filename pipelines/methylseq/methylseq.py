#!/usr/bin/env python

################################################################################
# Copyright (C) 2014, 2015 GenAP, McGill University and Genome Quebec Innovation Centre
#
# This file is part of MUGQIC Pipelines.
#
# MUGQIC Pipelines is free software: you can redistribute it and/or modify
# it under the terms of the GNU Lesser General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# MUGQIC Pipelines is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with MUGQIC Pipelines.  If not, see <http://www.gnu.org/licenses/>.
################################################################################

# Python Standard Modules
import logging
import math
import os
import re
import sys

# Append mugqic_pipelines directory to Python library path
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(sys.argv[0])))))

# MUGQIC Modules
from core.config import *
from core.job import *
from core.pipeline import *
from bfx.readset import *

from bfx import bvatools
from bfx import bismark
from bfx import picard2 as picard
from bfx import bedtools
from bfx import samtools
from bfx import gatk
from bfx import igvtools
from bfx import bissnp
from bfx import tools
from bfx import ucsc

from pipelines import common
from pipelines.dnaseq import dnaseq

log = logging.getLogger(__name__)

class MethylSeq(dnaseq.DnaSeq):
    """
    Methyl-Seq Pipeline
    ================

    The standard MUGQIC Methyl-Seq pipeline uses Bismark to align reads to the reference genome. Treatment
    and filtering of mapped reads approaches as mark duplicate reads, recalibration
    and sort are executed using Picard and GATK. Samtools MPILEUP and bcftools are used to produce
    the standard SNP and indels variants file (VCF). Additional SVN annotations mostly applicable
    to human samples include mappability flags, dbSNP annotation and extra information about SVN
    by using published databases.  The SNPeff tool is used to annotate variants using an integrated database
    of functional predictions from multiple algorithms (SIFT, Polyphen2, LRT and MutationTaster, PhyloP and GERP++, etc.)
    and to calculate the effects they produce on known genes.

    A summary html report is automatically generated by the pipeline. This report contains description
    of the sequencing experiment as well as a detailed presentation of the pipeline steps and results.
    Various Quality Control (QC) summary statistics are included in the report and additional QC analysis
    is accessible for download directly through the report. The report includes also the main references
    of the software and methods used during the analysis, together with the full list of parameters
    that have been passed to the pipeline main script.
    """

    @property
    def readsets(self):
        if not hasattr(self, "_readsets"):
            if self.args.readsets:
                self._readsets = parse_illumina_readset_file(self.args.readsets.name)
                for readset in self._readsets:
                    if readset._run == "":
                        raise Exception("Error: no run was provided for readset \"" + readset.name +
                            "\"... Run has to be provided for all the readsets in order to use this pipeline.")
                    if readset._lane == "":
                        raise Exception("Error: no lane provided for readset \"" + readset.name +
                            "\"... Lane has to be provided for all the readsets in order to use this pipeline.")
            else:
                self.argparser.error("argument -r/--readsets is required!")

        return self._readsets

    def bismark_align(self):
        """
        Align reads with Bismark
        """

        jobs = []
        for readset in self.readsets:
            trim_file_prefix = os.path.join("trim", readset.sample.name, readset.name + ".trim.")
            alignment_directory = os.path.join("alignment", readset.sample.name)
            no_readgroup_bam = os.path.join(alignment_directory, readset.name, readset.name + ".sorted_noRG.bam")
            output_bam = re.sub("_noRG.bam", ".bam", no_readgroup_bam)

            # Find input readset FASTQs first from previous trimmomatic job, then from original FASTQs in the readset sheet
            if readset.run_type == "PAIRED_END":
                candidate_input_files = [[trim_file_prefix + "pair1.fastq.gz", trim_file_prefix + "pair2.fastq.gz"]]
                if readset.fastq1 and readset.fastq2:
                    candidate_input_files.append([readset.fastq1, readset.fastq2])
                if readset.bam:
                    candidate_input_files.append([re.sub("\.bam$", ".pair1.fastq.gz", readset.bam), re.sub("\.bam$", ".pair2.fastq.gz", readset.bam)])
                [fastq1, fastq2] = self.select_input_files(candidate_input_files)
            elif readset.run_type == "SINGLE_END":
                candidate_input_files = [[trim_file_prefix + "single.fastq.gz"]]
                if readset.fastq1:
                    candidate_input_files.append([readset.fastq1])
                if readset.bam:
                    candidate_input_files.append([re.sub("\.bam$", ".single.fastq.gz", readset.bam)])
                [fastq1] = self.select_input_files(candidate_input_files)
                fastq2 = None
            else:
                raise Exception("Error: run type \"" + readset.run_type +
                "\" is invalid for readset \"" + readset.name + "\" (should be PAIRED_END or SINGLE_END)!")

            # Defining the bismark output files (bismark sets the names of its output files from the basename of fastq1)
            # Note : these files will then be renamed (using a "mv" command) to fit with the mugqic pipelines nomenclature (cf. no_readgroup_bam)
            bismark_out_bam = os.path.join(alignment_directory, readset.name, re.sub(r'(\.fastq\.gz|\.fq\.gz|\.fastq|\.fq)$', "_bismark_bt2_pe.bam", os.path.basename(fastq1)))
            bismark_out_report =  os.path.join(alignment_directory, readset.name, re.sub(r'(\.fastq\.gz|\.fq\.gz|\.fastq|\.fq)$', "_bismark_bt2_PE_report.txt", os.path.basename(fastq1)))

            jobs.append(
                concat_jobs([
                    Job(output_files=[output_bam], command="mkdir -p " + os.path.dirname(output_bam), samples=[readset.sample]),
                    bismark.align(
                        fastq1,
                        fastq2,
                        os.path.dirname(no_readgroup_bam),
                        [no_readgroup_bam, re.sub(".bam", "_bismark_bt2_PE_report.txt", no_readgroup_bam)],
                    ),
                    Job(command="mv " + bismark_out_bam + " " + no_readgroup_bam),
                    Job(command="mv " + bismark_out_report + " " + re.sub(".bam", "_bismark_bt2_PE_report.txt", no_readgroup_bam)),
                ], name="bismark_align." + readset.name)
            )
            jobs.append(
                concat_jobs([
                    Job(command="mkdir -p " + alignment_directory, samples=[readset.sample]),
                    picard.add_read_groups(
                        no_readgroup_bam,
                        output_bam,
                        readset.name,
                        readset.library if readset.library else readset.sample.name,
                        readset.run + "_" + readset.lane,
                        readset.sample.name
                    )
                ], name="picard_add_read_groups." + readset.name)
            )

        report_file = os.path.join("report", "MethylSeq.bismark_align.md")
        jobs.append(
            Job(
                [os.path.join("alignment", readset.sample.name, readset.name, readset.name + ".sorted.bam") for readset in self.readsets],
                [report_file],
                [['bismark_align', 'module_pandoc']],
                command="""\
mkdir -p report && \\
pandoc --to=markdown \\
  --template {report_template_dir}/{basename_report_file} \\
  --variable scientific_name="{scientific_name}" \\
  --variable assembly="{assembly}" \\
  {report_template_dir}/{basename_report_file} \\
  > {report_file}""".format(
                    scientific_name=config.param('bismark_align', 'scientific_name'),
                    assembly=config.param('bismark_align', 'assembly'),
                    report_template_dir=self.report_template_dir,
                    basename_report_file=os.path.basename(report_file),
                    report_file=report_file
                ),
                report_files=[report_file],
                name="bismark_align_report")
        )

        return jobs

    #def bismark_dedup(self):
        #"""
        #Remove duplicates reads with Bismark
        #"""

        ## Check the library status
        #library = {}
        #for readset in self.readsets:
            #if not library.has_key(readset.sample) :
                #library[readset.sample]="SINGLE_END"
            #if readset.run_type == "PAIRED_END" :
                #library[readset.sample]="PAIRED_END"

        #jobs = []
        #for sample in self.samples:
            #alignment_directory = os.path.join("alignment", sample.name)
            #bam_input = os.path.join(alignment_directory, sample.name + ".sorted.bam")
            #bam_readset_sorted = re.sub("sorted", "readset_sorted", bam_input)
            #dedup_bam_readset_sorted = re.sub(".bam", ".dedup.bam", bam_readset_sorted)
            #dedup_report = os.path.join(alignment_directory, sample.name + ".readset_sorted.deduplication_report.txt")
            #bam_output = os.path.join(alignment_directory, re.sub("readset_", "", os.path.basename(dedup_bam_readset_sorted)))

            #job = concat_jobs([
                #Job(command="mkdir -p " + alignment_directory),
                #picard.sort_sam(
                    #bam_input,
                    #bam_readset_sorted,
                    #"queryname"
                #),
                #bismark.dedup(
                    #bam_readset_sorted,
                    #[dedup_bam_readset_sorted, dedup_report],
                    #library[sample]
                #),
                #Job(command="mv " + re.sub(".bam", ".deduplicated.bam", bam_readset_sorted) + " " + dedup_bam_readset_sorted),
                #picard.sort_sam(
                    #dedup_bam_readset_sorted,
                    #bam_output
                #)
            #])
            #job.name = "bismark_dedup." + sample.name
            #job.removable_files = [dedup_bam_readset_sorted]
            #job.samples = [sample]

            #jobs.append(job)

        #report_file = os.path.join("report", "MethylSeq.bismark_dedup.md")
        #jobs.append(
            #Job(
                #[os.path.join("alignment", sample.name, sample.name + ".sorted.dedup.bam") for sample in self.samples],
                #[report_file],
                #command="""\
#mkdir -p report && \\
#cp \\
  #{report_template_dir}/{basename_report_file} \\
  #{report_file}""".format(
                    #report_template_dir=self.report_template_dir,
                    #basename_report_file=os.path.basename(report_file),
                    #report_file=report_file
                #),
                #report_files=[report_file],
                #name="bismark_dedup_report")
        #)


        #return jobs

    def picard_remove_duplicates(self):
        """
        Remove duplicates. Aligned reads per sample are duplicates if they have the same 5' alignment positions
        (for both mates in the case of paired-end reads). All but the best pair (based on alignment score)
        will be removed as a duplicate in the BAM file. Removing duplicates is done using [Picard](http://broadinstitute.github.io/picard/).
        """

        jobs = []
        for sample in self.samples:
            alignment_file_prefix = os.path.join("alignment", sample.name, sample.name + ".")
            input = alignment_file_prefix + "sorted.bam"
            #bam_readset_sorted = alignment_file_prefix + "readset_sorted.bam"
            bam_output = alignment_file_prefix + "sorted.dedup.bam"
            metrics_file = alignment_file_prefix + "sorted.dedup.metrics"
            dedup_bam_readset_sorted = alignment_file_prefix + "readset_sorted.dedup.bam"

            job = picard.mark_duplicates([input], bam_output, metrics_file, remove_duplicates="true")
            job.name = "picard_mark_duplicates." + sample.name
            jobs.append(job)
            job = picard.sort_sam(bam_output, dedup_bam_readset_sorted, "queryname")
            job.name = "picard_queryname_sort." + sample.name
            jobs.append(job)

        report_file = os.path.join("report", "methylseq.picard_remove_duplicates.md")
        jobs.append(
              Job(
                [os.path.join("alignment", sample.name, sample.name + ".sorted.dedup.bam") for sample in self.samples],
                [metrics_file],
                command="""\
mkdir -p report && \\
cp \\
  {report_template_dir}/{basename_report_file} \\
  {report_file}""".format(
                    report_template_dir=self.report_template_dir,
                    basename_report_file=os.path.basename(report_file),
                    report_file=metrics_file
                ),
                report_files=[metrics_file],
                name="picard_remove_duplicates_report")
        )

        return jobs


    def metrics(self):
        """
        Compute metrics and generate coverage tracks per sample. Multiple metrics are computed at this stage:
        Number of raw reads, Number of filtered reads, Number of aligned reads, Number of duplicate reads,
        Median, mean and standard deviation of insert sizes of reads after alignment, percentage of bases
        covered at X reads (%_bases_above_50 means the % of exons bases which have at least 50 reads)
        whole genome or targeted percentage of bases covered at X reads (%_bases_above_50 means the % of exons
        bases which have at least 50 reads). A TDF (.tdf) coverage track is also generated at this step
        for easy visualization of coverage in the IGV browser.
        """

        # check the library status
        library, bam = {}, {}
        for readset in self.readsets:
            if not library.has_key(readset.sample) :
                library[readset.sample]="SINGLE_END"
            if readset.run_type == "PAIRED_END" :
                library[readset.sample]="PAIRED_END"
            if not bam.has_key(readset.sample):
                bam[readset.sample]=""
            if readset.bam:
                bam[readset.sample]=readset.bam

        jobs = []
        created_interval_lists = []
        for sample in self.samples:
            file_prefix = os.path.join("alignment", sample.name, sample.name + ".sorted.dedup.")
            coverage_bed = bvatools.resolve_readset_coverage_bed(sample.readsets[0])

            candidate_input_files = [[file_prefix + "bam"]]
            if bam[sample]:
                candidate_input_files.append([bam[sample]])
            [input] = self.select_input_files(candidate_input_files)

            job = picard.collect_multiple_metrics(
                input,
                re.sub("bam", "all.metrics", input),
                library_type=library[sample]
            )
            job.name = "picard_collect_multiple_metrics." + sample.name
            job.samples = [sample]
            jobs.append(job)

            # Compute genome coverage with GATK
            job = gatk.depth_of_coverage(
                input,
                re.sub("bam", "all.coverage", input),
                coverage_bed
            )
            job.name = "gatk_depth_of_coverage.genome." + sample.name
            job.samples = [sample]
            jobs.append(job)

            # Compute genome or target coverage with BVATools
            job = bvatools.depth_of_coverage(
                input,
                re.sub("bam", "coverage.tsv", input),
                coverage_bed,
                other_options=config.param('bvatools_depth_of_coverage', 'other_options', required=False)
            )
            job.name = "bvatools_depth_of_coverage." + sample.name
            job.samples = [sample]
            jobs.append(job)

            if coverage_bed:
                # Get on-target reads (if on-target context is detected)
                ontarget_bam = re.sub("bam", "ontarget.bam", input)
                flagstat_output = re.sub("bam", "bam.flagstat", ontarget_bam)
                job = concat_jobs([
                    bedtools.intersect(
                        input,
                        ontarget_bam,
                        coverage_bed
                    ),
                    samtools.flagstat(
                        ontarget_bam,
                        flagstat_output
                    )
                ])
                job.name = "ontarget_reads." + sample.name
                job.removable_files=[ontarget_bam]
                job.samples = [sample]
                jobs.append(job)

                # Compute on target percent of hybridisation based capture
                interval_list = re.sub("\.[^.]+$", ".interval_list", coverage_bed)
                if not interval_list in created_interval_lists:
                    job = tools.bed2interval_list(None, coverage_bed, interval_list)
                    job.name = "interval_list." + os.path.basename(coverage_bed)
                    jobs.append(job)
                    created_interval_lists.append(interval_list)
                file_prefix = os.path.join("alignment", sample.name, sample.name + ".sorted.dedup.")
                job = picard.calculate_hs_metrics(file_prefix + "bam", file_prefix + "onTarget.tsv", interval_list)
                job.name = "picard_calculate_hs_metrics." + sample.name
                job.samples = [sample]
                jobs.append(job)

            # Calculate the number of reads with higher mapping quality than the threshold passed in the ini file
            job = concat_jobs([
                samtools.view(
                    input,
                    re.sub(".bam", ".filtered_reads.counts.txt", input),
                    "-c " + config.param('mapping_quality_filter', 'quality_threshold')
                )
            ])
            job.name = "mapping_quality_filter." + sample.name
            job.samples = [sample]
            jobs.append(job)

            # Calculate GC bias
            # For captured analysis
            #if coverage_bed:
                #target_input = re.sub(".bam", ".targeted.bam", input)
                #job = concat_jobs([
                    #bedtools.intersect(
                        #input,
                        #target_input,
                        #coverage_bed
                    #)
                    #bedtools.coverage(
                        #target_input,
                        #re.sub(".bam", ".gc_cov.1M.txt", target_input)
                    #),
                    #metrics.gc_bias(
                        #re.sub(".bam", ".gc_cov.1M.txt", target_input),
                        #re.sub(".bam", ".GCBias_all.txt", target_input)
                    #)
                #])
            # Or for whole genome analysis
            #else:
            gc_content_file = re.sub(".bam", ".gc_cov.1M.txt", input)
            job = bedtools.coverage(
                input,
                gc_content_file,
                coverage_bed
            )
            if coverage_bed:
                gc_content_on_target_file = re.sub(".bam", ".gc_cov.1M.on_target.txt", input)
                gc_ontent_target_job = bedtools.intersect(
                    gc_content_file,
                    gc_content_on_target_file,
                    coverage_bed
                )
                gc_content_file = gc_content_on_target_file
                job = concat_jobs([
                    job,
                    gc_ontent_target_job
                ])
            job = concat_jobs([
                job,
                metrics.gc_bias(
                    gc_content_file,
                    re.sub(".bam", ".GCBias_all.txt", input)
                )
            ])
            job.name = "GC_bias." + sample.name
            job.samples = [sample]
            jobs.append(job)

            job = igvtools.compute_tdf(input, input + ".tdf")
            job.name = "igvtools_compute_tdf." + sample.name
            job.samples = [sample]
            jobs.append(job)

        return jobs

    def methylation_call(self):
        """
        The script reads in a bisulfite read alignment file produced by the Bismark bisulfite mapper
        and extracts the methylation information for individual cytosines.
        The methylation extractor outputs result files for cytosines in CpG, CHG and CHH context.
        It also outputs bedGraph, a coverage file from positional methylation data and cytosine methylation report
        """

        # Check the library status
        library = {}
        for readset in self.readsets:
            if not library.has_key(readset.sample) :
                library[readset.sample]="SINGLE_END"
            if readset.run_type == "PAIRED_END" :
                library[readset.sample]="PAIRED_END"

        jobs = []
        for sample in self.samples:
            alignment_directory = os.path.join("alignment", sample.name)

            candidate_input_files = [[os.path.join(alignment_directory, sample.name + ".readset_sorted.dedup.bam")]]
            candidate_input_files.append([os.path.join(alignment_directory, sample.name + ".sorted.dedup.bam")])
            [input_file] = self.select_input_files(candidate_input_files)

            methyl_directory = os.path.join("methylation_call", sample.name)
            outputs = [
                os.path.join(methyl_directory, "CpG_context_" + re.sub( ".bam", ".txt.gz", os.path.basename(input_file))),
                os.path.join(methyl_directory, re.sub(".bam", ".bedGraph.gz", os.path.basename(input_file))),
                os.path.join(methyl_directory, re.sub(".bam", ".CpG_report.txt.gz", os.path.basename(input_file)))
            ]

            if input_file == os.path.join(alignment_directory, sample.name + ".readset_sorted.dedup.bam") :
                bismark_job = bismark.methyl_call(
                    input_file,
                    outputs,
                    library[sample]
                )
            else :
                outputs = [re.sub("sorted", "readset_sorted", output) for output in outputs]
                bismark_job = concat_jobs([
                    picard.sort_sam(
                        input_file,
                        re.sub("sorted", "readset_sorted", input_file),
                        "queryname"
                    ),
                    bismark.methyl_call(
                        re.sub("sorted", "readset_sorted", input_file),
                        outputs,
                        library[sample]
                    )
                ])

            jobs.append(
                concat_jobs([
                    Job(command="mkdir -p " + methyl_directory, samples=[sample]),
                    bismark_job
                ], name="bismark_methyl_call." + sample.name)
            )

        return jobs

    def wiggle_tracks(self):
        """
        Generate wiggle tracks suitable for multiple browsers, to show coverage and methylation data
        """

        jobs = []

        for sample in self.samples:
            alignment_directory = os.path.join("alignment", sample.name)

            # Generation of a bedGraph and a bigWig track to show the genome coverage
            candidate_input_files = [[os.path.join(alignment_directory, sample.name + ".sorted.dedup.bam")]]
            candidate_input_files.append([os.path.join(alignment_directory, sample.name + ".readset_sorted.dedup.bam")])

            [input_bam] = self.select_input_files(candidate_input_files)

            bed_graph_prefix = os.path.join("tracks", sample.name, sample.name)
            big_wig_prefix = os.path.join("tracks", "bigWig", sample.name)

            bed_graph_output = bed_graph_prefix + ".bedGraph"
            big_wig_output = big_wig_prefix + ".bw"

            if input_bam == os.path.join(alignment_directory, sample.name + ".readset_sorted.dedup.bam") :
                jobs.append(
                    concat_jobs([
                        Job(command="mkdir -p " + os.path.join("tracks", sample.name) + " " + os.path.join("tracks", "bigWig"), removable_files=["tracks"], samples=[sample]),
                        picard.sort_sam(
                            input_bam,
                            re.sub("readset_sorted", "sorted", input_bam),
                            "coordinate"
                        ),
                        bedtools.graph(re.sub("readset_sorted", "sorted", input_bam), bed_graph_output, big_wig_output, "")
                    ], name="wiggle." + re.sub(".bedGraph", "", os.path.basename(bed_graph_output)))
                )
            else :
                jobs.append(
                    concat_jobs([
                        Job(command="mkdir -p " + os.path.join("tracks", sample.name) + " " + os.path.join("tracks", "bigWig"), removable_files=["tracks"], samples=[sample]),
                        bedtools.graph(input_bam, bed_graph_output, big_wig_output, "")
                    ], name="wiggle." + re.sub(".bedGraph", "", os.path.basename(bed_graph_output)))
                )

            # Generation of a bigWig from the methylation bedGraph
            methyl_directory = os.path.join("methylation_call", sample.name)
            candidate_input_files = [[os.path.join(methyl_directory, sample.name + ".sorted.dedup.bedGraph.gz")]]
            candidate_input_files.append([os.path.join(methyl_directory, sample.name + ".readset_sorted.dedup.bedGraph.gz")])
            [input_bed_graph] = self.select_input_files(candidate_input_files)
            output_wiggle = os.path.join("tracks", "bigWig", re.sub(".bedGraph.gz", ".bw", os.path.basename(input_bed_graph)))

            jobs.append(
                concat_jobs([
                    Job(command="mkdir -p " + methyl_directory, samples=[sample]),
                    ucsc.bedGraphToBigWig(
                        input_bed_graph,
                        output_wiggle
                    )
                ], name = "bismark_bigWig." + sample.name)
            )

        return jobs

    def methylation_profile(self):
        """
        Generation of a CpG methylation profile by combining both forward and reverse strand Cs.
        Also generating of all the methylatoin metrics : CpG stats, pUC19 CpG stats, lambda conversion rate, median CpG coverage, GC bias
        """

        jobs = []
        for sample in self.samples:
            methyl_directory = os.path.join("methylation_call", sample.name)

            candidate_input_files = [[os.path.join(methyl_directory, sample.name + ".sorted.dedup.CpG_report.txt.gz")]]
            candidate_input_files.append([os.path.join(methyl_directory, sample.name + ".readset_sorted.dedup.CpG_report.txt.gz")])

            [cpg_input_file] = self.select_input_files(candidate_input_files)
            cpg_profile = re.sub(".CpG_report.txt.gz", ".CpG_profile.strand.combined.csv", cpg_input_file)

            # Generate CpG methylation profile
            job = tools.bismark_combine(
                cpg_input_file,
                cpg_profile
            )
            job.name = "methylation_profile." + sample.name
            job.samples = [sample]
            jobs.append(job)

            # Generate stats for lambda, pUC19 and regular CpGs
            cg_stats_output = re.sub(".CpG_report.txt.gz", ".profile.cgstats.txt", cpg_input_file)
            lambda_stats_output = re.sub(".CpG_report.txt.gz", ".profile.lambda.conversion.rate.tsv", cpg_input_file)
            puc19_stats_output = re.sub(".CpG_report.txt.gz", ".profile.pUC19.txt", cpg_input_file)
            job = tools.cpg_stats(
                cpg_profile,
                cg_stats_output,
                lambda_stats_output,
                puc19_stats_output
            )
            job.name = "CpG_stats." + sample.name
            job.samples = [sample]
            jobs.append(job)

            target_bed = bvatools.resolve_readset_coverage_bed(sample.readsets[0])
            if target_bed:
                # Create targeted combined file
                target_cpg_profile = re.sub("combined", "combined.on_target", cpg_profile)
                job = bedtools.intersect(
                    cpg_profile,
                    target_cpg_profile,
                    target_bed
                )
                job.name = "extract_target_CpG_profile." + sample.name
                job.samples = [sample]
                jobs.append(job)
                cpg_profile = target_cpg_profile

            # Caluculate median & mean CpG coverage
            median_CpG_coverage = re.sub(".CpG_report.txt.gz", ".median_CpG_coverage.txt", cpg_input_file)
            job = tools.cpg_cov_stats(
                cpg_profile,
                median_CpG_coverage
            )
            job.name = "median_CpG_coverage." + sample.name
            job.samples = [sample]
            if target_bed:
                job.removable_files = [target_cpg_profile]
            jobs.append(job)

        return jobs

    def all_sample_metrics_report(self):
        """
        Retrieve all the computed metrics (alignment metrics as well as methylation metrics) to build a tsv report table
        """

        jobs = []

        target_bed = bvatools.resolve_readset_coverage_bed(self.samples[0].readsets[0])
        metrics_file = os.path.join("metrics", "sampleMetrics.stats")
        report_metrics_file = os.path.join("report", "sampleMetricsTable.tsv")

        if target_bed:
            report_file = os.path.join("report", "MethylSeq.all_sample_metrics_targeted_report.md")
        else:
            report_file = os.path.join("report", "MethylSeq.all_sample_metrics_report.md")

        # Create the list of input files to handle job dependencies
        inputs = []
        sample_list = []
        for sample in self.samples:
            sample_list.append(sample.name)

            # Trim log files
            for readset in sample.readsets:
                inputs.append(os.path.join("trim", sample.name, readset.name + ".trim.log"))

            # Dedupplication report files
            inputs.append(os.path.join("alignment", sample.name, sample.name + ".sorted.dedup.metrics"))

            # Coverage summary files
            inputs.append(os.path.join("alignment", sample.name, sample.name + ".sorted.dedup.all.coverage.sample_summary"))

            # Lambda conversion rate files
            [lambda_conv_file] = self.select_input_files([
                [os.path.join("methylation_call", sample.name, sample.name + ".sorted.dedup.profile.lambda.conversion.rate.tsv")],
                [os.path.join("methylation_call", sample.name, sample.name + ".readset_sorted.dedup.profile.lambda.conversion.rate.tsv")]
            ])
            inputs.append(lambda_conv_file)

            # CG stat files
            [cgstats_file] = self.select_input_files([
                [os.path.join("methylation_call", sample.name, sample.name + ".sorted.dedup.profile.cgstats.txt")],
                [os.path.join("methylation_call", sample.name, sample.name + ".readset_sorted.dedup.profile.cgstats.txt")]
            ])
            inputs.append(cgstats_file)

            # Flagstat file if in targeted context
            if target_bed : inputs.append(os.path.join("alignment", sample.name, sample.name + ".sorted.dedup.ontarget.bam.flagstat"))

        jobs.append(
            concat_jobs([
                Job(command="mkdir -p metrics"),
                tools.methylseq_metrics_report(sample_list, inputs, metrics_file, target_bed),
                Job(
                    [metrics_file],
                    [report_file],
                    [['all_sample_metrics_report', 'module_pandoc']],
                    command="""\
mkdir -p report && \\
cp {metrics_file} {report_metrics_file} && \\
metrics_table_md=`sed 's/\t/|/g' {report_metrics_file}`
pandoc \\
  {report_template_dir}/{basename_report_file} \\
  --template {report_template_dir}/{basename_report_file} \\
  --variable sequence_alignment_table="$metrics_table_md" \\
  --to markdown \\
  > {report_file}""".format(
                        report_template_dir=self.report_template_dir,
                        metrics_file=metrics_file,
                        basename_report_file=os.path.basename(report_file),
                        report_metrics_file=report_metrics_file,
                        report_file=report_file
                    ),
                    report_files=[report_file]
                )
            ], name="all_sample_metrics_report")
        )

        return jobs

    def ihec_sample_metrics_report(self):
        """
        Retrieve the computed metrics which fit the IHEC standards and build a tsv report table for IHEC
        """

        jobs = []

        target_bed = bvatools.resolve_readset_coverage_bed(self.samples[0].readsets[0])
        metrics_all_file = os.path.join("metrics", "IHEC.sampleMetrics.stats")
        report_metrics_file = os.path.join("report", "IHEC.sampleMetricsTable.tsv")

        if target_bed:
            report_file = os.path.join("report", "MethylSeq.ihec_sample_metrics_targeted_report.md")
        else:
            report_file = os.path.join("report", "MethylSeq.ihec_sample_metrics_report.md")

        # Create the list of input files to handle job dependencies
        inputs = []
        sample_list = []
        counter=0
        for sample in self.samples:
            sample_list.append(sample.name)
            metrics_file =  os.path.join("ihec_metrics", sample.name + ".read_stats.txt")
            # Trim log files
            for readset in sample.readsets:
                inputs.append(os.path.join("trim", sample.name, readset.name + ".trim.log"))

            # Dedupplication report files
            inputs.append(os.path.join("alignment", sample.name, sample.name + ".sorted.dedup.metrics"))

            # Coverage summary files
            inputs.append(os.path.join("alignment", sample.name, sample.name + ".sorted.dedup.all.coverage.sample_summary"))

            # Filtered reads count files
            inputs.append(os.path.join("alignment", sample.name, sample.name + ".sorted.dedup.filtered_reads.counts.txt"))

            # GC bias files
            inputs.append(os.path.join("alignment", sample.name, sample.name + ".sorted.dedup.GCBias_all.txt"))

            # Bismark alignment files
            for readset in sample.readsets:
                inputs.append(os.path.join("alignment", sample.name, readset.name, readset.name + ".sorted_noRG_bismark_bt2_PE_report.txt"))

            # CpG coverage files
            inputs.append(os.path.join("methylation_call", sample.name, sample.name + ".readset_sorted.dedup.median_CpG_coverage.txt"))

            # pUC19 methylation files
            inputs.append(os.path.join("methylation_call", sample.name, sample.name + ".readset_sorted.dedup.profile.pUC19.txt"))

            # Lambda conversion rate files
            [lambda_conv_file] = self.select_input_files([
                [os.path.join("methylation_call", sample.name, sample.name + ".sorted.dedup.profile.lambda.conversion.rate.tsv")],
                [os.path.join("methylation_call", sample.name, sample.name + ".readset_sorted.dedup.profile.lambda.conversion.rate.tsv")]
            ])
            inputs.append(lambda_conv_file)

            # CG stat files
            [cgstats_file] = self.select_input_files([
                [os.path.join("methylation_call", sample.name, sample.name + ".sorted.dedup.profile.cgstats.txt")],
                [os.path.join("methylation_call", sample.name, sample.name + ".readset_sorted.dedup.profile.cgstats.txt")]
            ])
            inputs.append(cgstats_file)

            # Flagstat file if in targeted context
            if target_bed : inputs.append(os.path.join("alignment", sample.name, sample.name + ".sorted.dedup.ontarget.bam.flagstat"))

            jobs.append(
            concat_jobs([
                Job(command="mkdir -p ihec_metrics metrics"),
                tools.methylseq_ihec_metrics_report(sample.name, inputs, metrics_file, metrics_all_file, target_bed, counter),
            ], name="ihec_sample_metrics_report")
            )
            counter+=1


        jobs.append(
            concat_jobs([
                Job(command="mkdir -p metrics"),
                Job(
                    [metrics_all_file],
                    [report_file],
                    [['ihec_sample_metrics_report', 'module_pandoc']],
                    command="""\
mkdir -p report && \\
cp {metrics_file} {report_metrics_file} && \\
metrics_table_md=`sed 's/\t/|/g' {report_metrics_file}`
pandoc \\
  {report_template_dir}/{basename_report_file} \\
  --template {report_template_dir}/{basename_report_file} \\
  --variable sequence_alignment_table="$metrics_table_md" \\
  --to markdown \\
  > {report_file}""".format(
                        report_template_dir=self.report_template_dir,
                        metrics_file=metrics_file,
                        basename_report_file=os.path.basename(report_file),
                        report_metrics_file=report_metrics_file,
                        report_file=report_file
                    ),
                    report_files=[report_file]
                )
            ], name="ihec_sample_metrics_report")
        )

        return jobs

    def bis_snp(self):
        """
        SNP calling with BisSNP
        """

        jobs = []
        for sample in self.samples:
            alignment_directory = os.path.join("alignment", sample.name)

            candidate_input_files = [[os.path.join(alignment_directory, sample.name + ".sorted.dedup.bam")]]
            candidate_input_files.append([os.path.join(alignment_directory, sample.name + ".readset_sorted.dedup.bam")])
            [input_file] = self.select_input_files(candidate_input_files)

            variant_directory = os.path.join("variants", sample.name)
            cpg_output_file = os.path.join(variant_directory, sample.name + ".cpg.vcf")
            snp_output_file = os.path.join(variant_directory, sample.name + ".snp.vcf")

            jobs.append(
                concat_jobs([
                    Job(command="mkdir -p " + variant_directory, samples=[sample]),
                    bissnp.bisulfite_genotyper(
                        input_file,
                        cpg_output_file,
                        snp_output_file
                    )
                ], name="bissnp." + sample.name)
            )

        return jobs 

    @property
    def steps(self):
        return [
            self.picard_sam_to_fastq,
            self.trimmomatic,
            self.merge_trimmomatic_stats,
            self.bismark_align,
            self.picard_merge_sam_files,    # step 5
#            self.bismark_dedup,
            self.picard_remove_duplicates,
            self.metrics,
            self.verify_bam_id,
            self.methylation_call,
            self.wiggle_tracks,             # step 10
            self.methylation_profile,
            self.all_sample_metrics_report,
            self.ihec_sample_metrics_report,
            self.bis_snp                    # step 14
        ]

if __name__ == '__main__': 
    MethylSeq()
